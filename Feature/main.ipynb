{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from DevideDataSet.ipynb\n",
      "importing Jupyter notebook from FeatureConstruct.ipynb\n",
      "importing Jupyter notebook from KMeansPreprocessing.ipynb\n",
      "importing Jupyter notebook from TrainModel.ipynb\n",
      "importing Jupyter notebook from Predict.ipynb\n",
      "importing Jupyter notebook from Validate.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "#self function\n",
    "import Ipynb_importer\n",
    "import DevideDataSet\n",
    "import FeatureConstruct\n",
    "import KMeansPreprocessing\n",
    "import TrainModel\n",
    "import Predict\n",
    "import Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征窗口第1次滑动 特征窗口大小为：7 特征窗口总滑动次数：1\n",
      "start_day: 2014-12-12 00:00:00 \n",
      " end_day: 2014-12-18 00:00:00\n",
      "output files name,as follows:\n",
      " ../DataSet/DevideData/part_data2014-12-12to2014-12-17.csv \n",
      " ../DataSet/DevideData/part_data2014-12-18.csv \n",
      " ../DataSet/DevideData/part_data_uicl2014-12-18.csv\n",
      "get uic label data from 2014-12-12  to 2014-12-18 start time: 2019-05-25 01:21:52\n",
      "chunk 1 done.\n",
      "chunk 2 done.\n",
      "chunk 3 done.\n",
      "end time: 2019-05-25 01:25:26\n",
      "feature construct from 2014-12-12  to 2014-12-18 start time: 2019-05-25 01:25:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FeatureConstruct.ipynb:280: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"    ui_b_count_num['ui_b_count']= ui_b_count_num['ui_b1_count'] + ui_b_count_num['ui_b2_count'] + ui_b_count_num['ui_b3_count'] + ui_b_count_num['ui_b4_count']\\n\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature construct  time: 2019-05-25 01:29:52\n",
      "feature construct join time: 2019-05-25 01:29:56\n",
      "end time: 2019-05-25 01:30:07\n",
      "syj predict_set Index(['user_id', 'item_id', 'item_category', 'label', 'u_b1_count_x',\n",
      "       'u_b2_count_x', 'u_b3_count_x', 'u_b4_count_x', 'u_b_count_x',\n",
      "       'u_b4_rate_x',\n",
      "       ...\n",
      "       'uc_b1_count', 'uc_b2_count', 'uc_b3_count', 'uc_b4_count',\n",
      "       'uc_b_count', 'uc_b_count_rank_in_u', 'uc_b1_last_hours',\n",
      "       'uc_b2_last_hours', 'uc_b3_last_hours', 'uc_b4_last_hours'],\n",
      "      dtype='object', length=145)\n",
      "syj predict_set_temp Index(['label', 'u_b1_count_x', 'u_b2_count_x', 'u_b3_count_x', 'u_b4_count_x',\n",
      "       'u_b_count_x', 'u_b4_rate_x', 'u_b4_diff_hours_x', 'u_b1_count_y',\n",
      "       'u_b2_count_y',\n",
      "       ...\n",
      "       'uc_b1_count', 'uc_b2_count', 'uc_b3_count', 'uc_b4_count',\n",
      "       'uc_b_count', 'uc_b_count_rank_in_u', 'uc_b1_last_hours',\n",
      "       'uc_b2_last_hours', 'uc_b3_last_hours', 'uc_b4_last_hours'],\n",
      "      dtype='object', length=142)\n",
      "syj predict_set_temp    label  u_b1_count_x  u_b2_count_x  u_b3_count_x  u_b4_count_x  u_b_count_x  \\\n",
      "0      0          33.0           0.0           0.0           1.0         34.0   \n",
      "1      0          33.0           0.0           0.0           1.0         34.0   \n",
      "2      0          33.0           0.0           0.0           1.0         34.0   \n",
      "3      0          33.0           0.0           0.0           1.0         34.0   \n",
      "4      0          33.0           0.0           0.0           1.0         34.0   \n",
      "\n",
      "   u_b4_rate_x  u_b4_diff_hours_x  u_b1_count_y  u_b2_count_y  \\\n",
      "0     0.029412                0.0           4.0           0.0   \n",
      "1     0.029412                0.0           4.0           0.0   \n",
      "2     0.029412                0.0           4.0           0.0   \n",
      "3     0.029412                0.0           4.0           0.0   \n",
      "4     0.029412                0.0           4.0           0.0   \n",
      "\n",
      "         ...         uc_b1_count  uc_b2_count  uc_b3_count  uc_b4_count  \\\n",
      "0        ...                -1.0         -1.0         -1.0         -1.0   \n",
      "1        ...                -1.0         -1.0         -1.0         -1.0   \n",
      "2        ...                -1.0         -1.0         -1.0         -1.0   \n",
      "3        ...                -1.0         -1.0         -1.0         -1.0   \n",
      "4        ...                -1.0         -1.0         -1.0         -1.0   \n",
      "\n",
      "   uc_b_count  uc_b_count_rank_in_u  uc_b1_last_hours  uc_b2_last_hours  \\\n",
      "0        -1.0                  -1.0              -1.0              -1.0   \n",
      "1        -1.0                  -1.0              -1.0              -1.0   \n",
      "2        -1.0                  -1.0              -1.0              -1.0   \n",
      "3        -1.0                  -1.0              -1.0              -1.0   \n",
      "4        -1.0                  -1.0              -1.0              -1.0   \n",
      "\n",
      "   uc_b3_last_hours  uc_b4_last_hours  \n",
      "0              -1.0              -1.0  \n",
      "1              -1.0              -1.0  \n",
      "2              -1.0              -1.0  \n",
      "3              -1.0              -1.0  \n",
      "4              -1.0              -1.0  \n",
      "\n",
      "[5 rows x 142 columns]\n",
      "train model predict start time: 2019-05-25 01:33:47\n",
      "loading model name ../ModelFile/GBDT_train_model.m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 141 and input n_features is 142 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-742e5a7b7570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mfeature_window_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mFeatureConstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_window_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mPredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GBDT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MachineLearning/github/MobileRecommendation2/Feature/Predict.ipynb\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(start_day, end_day, algorithm, use_predict_proba, flag)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   2035\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \"\"\"\n\u001b[0;32m-> 2037\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0mdecisions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_score_to_decision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecisions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1990\u001b[0m         \"\"\"\n\u001b[1;32m   1991\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1992\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# for use in inner loop, not raveling the output in single-class case,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0;31m# not doing input validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m         \u001b[0mpredict_stages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_init_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1574\u001b[0m         \u001b[0;34m\"\"\"Check input and compute prediction of ``init``. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m             raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    386\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 141 and input n_features is 142 "
     ]
    }
   ],
   "source": [
    "predict_set_flag = True\n",
    "\n",
    "#predict_set_flag = False #means get train data set\n",
    "end = datetime.datetime(2014,12,5,0,0,0)\n",
    "if __name__ == '__main__':\n",
    "    ##根据提取特征的时间范围的不同，特征提取窗口的滑动次数也是会相应变化的。\n",
    "    ##另：以双十二为分界线分别单独提取特征;还是以忽略的方式，有待确定（验证）\n",
    "    window_size = 7#提取特征的时间范围，单位是天\n",
    "    if(predict_set_flag):\n",
    "        window_slide_num=2\n",
    "        window_size = 7#提取特征的时间范围，单位是天。由7改为6的目的是避免碰上12月12日的数据\n",
    "    else:\n",
    "        window_slide_num = 4#特征提取窗口的滑动次数\n",
    "    #for i in list(range(window_size))[1:]:\n",
    "    k=1\n",
    "    for i in list(range(window_slide_num))[1:]:\n",
    "        print('特征窗口第%d次滑动'%(i),\"特征窗口大小为：%d\"%(window_size),\"特征窗口总滑动次数：%d\"%(window_slide_num-1))\n",
    "        if(predict_set_flag):\n",
    "            end = datetime.datetime(2014,12,18,0,0,0)\n",
    "            start = end - datetime.timedelta(days=window_size-1)   \n",
    "            print('start_day: %s'%(str(start)),'\\n end_day: %s'%(str(end)))\n",
    "            DevideDataSet.devide_data(\"../DataSet/tianchi_fresh_comp_train_user.csv\", start, end, flag=False) #修改日期时注意这个True、False的选择\n",
    "            feature_window_size = [4,2,1]\n",
    "            FeatureConstruct.feature_construct(start,end,feature_window_size)\n",
    "            Predict.predict(start, end, 'GBDT', False)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            start_time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"第 %d 轮 start time：\"%(i), start_time,'\\n')\n",
    "            feature_window_size =[4,2,1]  # 源代码中写死的状态\n",
    "            start = end - datetime.timedelta(days=window_size-1)   \n",
    "            print('start_day: %s'%(str(start)),'\\n end_day: %s'%(str(end)))\n",
    "            DevideDataSet.devide_data(\"../DataSet/tianchi_fresh_comp_train_user.csv\", start, end)\n",
    "            FeatureConstruct.feature_construct(start,end,feature_window_size)\n",
    "            KMeansPreprocessing.kmeans_preprocessing(start,end) \n",
    "            end_time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"第 %d 轮 end time：\"%(i), end_time,'\\n')\n",
    "                \n",
    "        end = end - datetime.timedelta(days=window_size)        \n",
    "        if(end <= datetime.datetime(2014,11,22,0,0,0)):\n",
    "            break\n",
    "    if(predict_set_flag):\n",
    "        pass\n",
    "    else:\n",
    "        train_data_filename = \"../DataSet/Kmeans/\"+\"train_data_all\"+\".csv\"\n",
    "        TrainModel.train_model(train_data_filename, 'GBDT')      #train_data_filename wait to add \n",
    "    print('main function end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3eb6f0bb73be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../DataSet/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"train_data_all\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTrainModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GBDT'\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#train_data_filename wait to add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TrainModel' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_filename = \"../DataSet/\"+\"train_data_all\"+\".csv\"\n",
    "TrainModel.train_model(train_data_filename, 'GBDT')      #train_data_filename wait to add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference_data_set_buy: 730\n",
      "predict_data_set_buy: 2919\n",
      "right_res: 54\n",
      "P: 0.018499486125385406 \n",
      " R: 0.07397260273972603 \n",
      " F1: 0.029597149904083308\n"
     ]
    }
   ],
   "source": [
    "Validate.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "GBDT\n",
      "train model start time: 2019-04-08 09:06:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sangyongjia/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.1783           0.0050            2.01m\n",
      "         2           0.1751           0.0036            1.72m\n",
      "         3           0.1712           0.0031            1.67m\n",
      "         4           0.1701           0.0020            1.61m\n",
      "         5           0.1686           0.0024            1.58m\n",
      "         6           0.1655           0.0018            1.56m\n",
      "         7           0.1624           0.0018            1.54m\n",
      "         8           0.1631           0.0017            1.52m\n",
      "         9           0.1615           0.0014            1.51m\n",
      "        10           0.1592           0.0013            1.49m\n",
      "        20           0.1511           0.0007            1.42m\n",
      "        30           0.1433           0.0004            1.40m\n",
      "        40           0.1407           0.0003            1.36m\n",
      "        50           0.1380           0.0002            1.33m\n",
      "        60           0.1374           0.0001            1.36m\n",
      "        70           0.1352           0.0001            1.31m\n",
      "        80           0.1336           0.0001            1.26m\n",
      "        90           0.1331           0.0000            1.22m\n",
      "       100           0.1326           0.0000            1.19m\n",
      "       200           0.1284           0.0000           51.76s\n",
      "       300           0.1276          -0.0000           35.44s\n",
      "train model finish time: 2019-04-08 09:07:29\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.025, loss='deviance', max_depth=4,\n",
      "              max_features='sqrt', max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=10, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              n_iter_no_change=50, presort='auto', random_state=None,\n",
      "              subsample=0.8, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=True, warm_start=False)\n",
      "GBDT model save...\n",
      "train model finished\n"
     ]
    }
   ],
   "source": [
    "train_data_filename = \"../DataSet/\"+\"train_data_all\"+\".csv\"\n",
    "TrainModel.train_model(train_data_filename, 'GBDT')      #train_data_filename wait to add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syj predict_set Index(['user_id', 'item_id', 'item_category', 'u_b1_count_x', 'u_b2_count_x',\n",
      "       'u_b3_count_x', 'u_b4_count_x', 'u_b_count_x', 'u_b1_count_y',\n",
      "       'u_b2_count_y', 'u_b3_count_y', 'u_b4_count_y', 'u_b_count_y',\n",
      "       'u_b1_count', 'u_b2_count', 'u_b3_count', 'u_b4_count', 'u_b_count',\n",
      "       'i_b1_count_x', 'i_b2_count_x', 'i_b3_count_x', 'i_b4_count_x',\n",
      "       'i_b_count_x', 'i_b4_rate_x', 'i_user_num_x', 'i_b1_count_y',\n",
      "       'i_b2_count_y', 'i_b3_count_y', 'i_b4_count_y', 'i_b_count_y',\n",
      "       'i_b4_rate_y', 'i_user_num_y', 'i_b1_count', 'i_b2_count', 'i_b3_count',\n",
      "       'i_b4_count', 'i_b_count', 'i_b4_rate', 'i_user_num', 'c_b1_count_x',\n",
      "       'c_b2_count_x', 'c_b3_count_x', 'c_b4_count_x', 'c_b_count_x',\n",
      "       'c_b4_rate_x', 'c_user_num_x', 'c_b1_count_y', 'c_b2_count_y',\n",
      "       'c_b3_count_y', 'c_b4_count_y', 'c_b_count_y', 'c_b4_rate_y',\n",
      "       'c_user_num_y', 'c_b1_count', 'c_b2_count', 'c_b3_count', 'c_b4_count',\n",
      "       'c_b_count', 'c_b4_rate', 'c_user_num', 'ic_sales_num_x',\n",
      "       'ic_user_num_x', 'ic_behavior_num_x', 'ic_sales_num_y', 'ic_user_num_y',\n",
      "       'ic_behavior_num_y', 'ic_sales_num', 'ic_user_num', 'ic_behavior_num',\n",
      "       'ui_sort_x', 'ui_b1_count_x', 'ui_b2_count_x', 'ui_b3_count_x',\n",
      "       'ui_b4_count_x', 'ui_cumcount_x', 'ui_b_count_x', 'ui_sort_y',\n",
      "       'ui_b1_count_y', 'ui_b2_count_y', 'ui_b3_count_y', 'ui_b4_count_y',\n",
      "       'ui_cumcount_y', 'ui_b_count_y', 'ui_sort', 'ui_b1_count',\n",
      "       'ui_b2_count', 'ui_b3_count', 'ui_b4_count', 'ui_cumcount',\n",
      "       'ui_b_count'],\n",
      "      dtype='object')\n",
      "syj predict_set_temp Index(['u_b1_count_x', 'u_b2_count_x', 'u_b3_count_x', 'u_b4_count_x',\n",
      "       'u_b_count_x', 'u_b1_count_y', 'u_b2_count_y', 'u_b3_count_y',\n",
      "       'u_b4_count_y', 'u_b_count_y', 'u_b1_count', 'u_b2_count', 'u_b3_count',\n",
      "       'u_b4_count', 'u_b_count', 'i_b1_count_x', 'i_b2_count_x',\n",
      "       'i_b3_count_x', 'i_b4_count_x', 'i_b_count_x', 'i_b4_rate_x',\n",
      "       'i_user_num_x', 'i_b1_count_y', 'i_b2_count_y', 'i_b3_count_y',\n",
      "       'i_b4_count_y', 'i_b_count_y', 'i_b4_rate_y', 'i_user_num_y',\n",
      "       'i_b1_count', 'i_b2_count', 'i_b3_count', 'i_b4_count', 'i_b_count',\n",
      "       'i_b4_rate', 'i_user_num', 'c_b1_count_x', 'c_b2_count_x',\n",
      "       'c_b3_count_x', 'c_b4_count_x', 'c_b_count_x', 'c_b4_rate_x',\n",
      "       'c_user_num_x', 'c_b1_count_y', 'c_b2_count_y', 'c_b3_count_y',\n",
      "       'c_b4_count_y', 'c_b_count_y', 'c_b4_rate_y', 'c_user_num_y',\n",
      "       'c_b1_count', 'c_b2_count', 'c_b3_count', 'c_b4_count', 'c_b_count',\n",
      "       'c_b4_rate', 'c_user_num', 'ic_sales_num_x', 'ic_user_num_x',\n",
      "       'ic_behavior_num_x', 'ic_sales_num_y', 'ic_user_num_y',\n",
      "       'ic_behavior_num_y', 'ic_sales_num', 'ic_user_num', 'ic_behavior_num',\n",
      "       'ui_sort_x', 'ui_b1_count_x', 'ui_b2_count_x', 'ui_b3_count_x',\n",
      "       'ui_b4_count_x', 'ui_cumcount_x', 'ui_b_count_x', 'ui_sort_y',\n",
      "       'ui_b1_count_y', 'ui_b2_count_y', 'ui_b3_count_y', 'ui_b4_count_y',\n",
      "       'ui_cumcount_y', 'ui_b_count_y', 'ui_sort', 'ui_b1_count',\n",
      "       'ui_b2_count', 'ui_b3_count', 'ui_b4_count', 'ui_cumcount',\n",
      "       'ui_b_count'],\n",
      "      dtype='object')\n",
      "syj predict_set_temp    u_b1_count_x  u_b2_count_x  u_b3_count_x  u_b4_count_x  u_b_count_x  \\\n",
      "0            51             0             0             1           52   \n",
      "1            51             0             0             1           52   \n",
      "2            51             0             0             1           52   \n",
      "3            51             0             0             1           52   \n",
      "4            51             0             0             1           52   \n",
      "\n",
      "   u_b1_count_y  u_b2_count_y  u_b3_count_y  u_b4_count_y  u_b_count_y  \\\n",
      "0            51             0             0             1           52   \n",
      "1            51             0             0             1           52   \n",
      "2            51             0             0             1           52   \n",
      "3            51             0             0             1           52   \n",
      "4            51             0             0             1           52   \n",
      "\n",
      "      ...      ui_b4_count_y  ui_cumcount_y  ui_b_count_y  ui_sort  \\\n",
      "0     ...               -1.0           -1.0          -1.0     -1.0   \n",
      "1     ...                0.0           14.0          14.0     23.0   \n",
      "2     ...                1.0            4.0           4.0     22.0   \n",
      "3     ...                0.0            1.0           1.0      9.0   \n",
      "4     ...                0.0            3.0           3.0     17.0   \n",
      "\n",
      "   ui_b1_count  ui_b2_count  ui_b3_count  ui_b4_count  ui_cumcount  ui_b_count  \n",
      "0         -1.0         -1.0         -1.0         -1.0         -1.0        -1.0  \n",
      "1         14.0          0.0          0.0          0.0         14.0        14.0  \n",
      "2          3.0          0.0          0.0          1.0          4.0         4.0  \n",
      "3          1.0          0.0          0.0          0.0          1.0         1.0  \n",
      "4          3.0          0.0          0.0          0.0          3.0         3.0  \n",
      "\n",
      "[5 rows x 87 columns]\n",
      "train model predict start time: 2019-04-08 19:16:13\n",
      "loading model name ../ModelFile/GBDT_train_model.m\n",
      "train model predict finish time: 2019-04-08 19:16:40\n",
      "before predict: (1555260, 87) after predict (1555260, 1)\n",
      "GBDT end\n",
      "reference_data_set_buy: 730\n",
      "predict_data_set: 1555260\n",
      "predict_data_set_buy: 2919\n",
      "right_res: 54\n",
      "P: 0.018499486125385406 \n",
      " R: 0.07397260273972603 \n",
      " F1: 0.029597149904083308\n"
     ]
    }
   ],
   "source": [
    "#import Predict\n",
    "window_size=6\n",
    "end = datetime.datetime(2014,12,18,0,0,0)\n",
    "start = end - datetime.timedelta(days=window_size-1)      \n",
    "Predict.predict(start, end, 'GBDT', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextFileReader' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6a0195cd3b77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../DataSet/Kmeans/train_data_all.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextFileReader' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "a = pd.read_csv(\"../DataSet/Kmeans/train_data_all.csv\", chunksize = 100000,index_col = False)\n",
    "a.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    250446  1016  40  28   4  1088  1016.1  40.1  28.1  4.1  ...   6.4  6.5  \\\n",
      "0     5176  1244  38   0  14  1296    1244    38     0   14  ...     6    6   \n",
      "1  1617346   804   0  14  14   832     804     0    14   14  ...     6    6   \n",
      "2  1366853  1394   2  18   0  1414    1394     2    18    0  ...     6    6   \n",
      "3  1587480  1498   0  36   0  1534    1498     0    36    0  ...     6    6   \n",
      "4   405535  1262  58  22   4  1346    1262    58    22    4  ...     6    6   \n",
      "\n",
      "   122.2  6.6  0.15  0.16  0.17  6.7  6.8  0.18  \n",
      "0    201    6     0     0     0    6    6   0.0  \n",
      "1    143    6     0     0     0    6    6   0.0  \n",
      "2    175    6     0     0     0    6    6   0.0  \n",
      "3    239    6     0     0     0    6    6   0.0  \n",
      "4    170    6     0     0     0    6    6   0.0  \n",
      "\n",
      "[5 rows x 89 columns]\n",
      "Index(['250446', '1016', '40', '28', '4', '1088', '1016.1', '40.1', '28.1',\n",
      "       '4.1', '1088.1', '1016.2', '40.2', '28.2', '4.2', '1088.2', '36', '0',\n",
      "       '0.1', '0.2', '36.1', '0.0', '4.3', '36.2', '0.3', '0.4', '0.5', '36.3',\n",
      "       '0.0.1', '4.4', '36.4', '0.6', '0.7', '0.8', '36.5', '0.0.2', '4.5',\n",
      "       '269306', '5794', '4624', '640', '280364', '0.002282746714984805',\n",
      "       '6044', '269306.1', '5794.1', '4624.1', '640.1', '280364.1',\n",
      "       '0.002282746714984805.1', '6044.1', '269306.2', '5794.2', '4624.2',\n",
      "       '640.2', '280364.2', '0.002282746714984805.2', '6044.2', '0.0.3', '4.6',\n",
      "       '36.6', '0.0.4', '4.7', '36.7', '0.0.5', '4.8', '36.8', '122', '6',\n",
      "       '0.9', '0.10', '0.11', '6.1', '6.2', '122.1', '6.3', '0.12', '0.13',\n",
      "       '0.14', '6.4', '6.5', '122.2', '6.6', '0.15', '0.16', '0.17', '6.7',\n",
      "       '6.8', '0.18'],\n",
      "      dtype='object')\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "b=0\n",
    "\n",
    "for test_df in pd.read_csv(\"../DataSet/train_data_all.csv\", chunksize = 100000,index_col = False):  \n",
    "    print(test_df.head(5))\n",
    "    print(test_df.columns)\n",
    "    #print(test_df.columns[2])\n",
    "    #print(test_df.columns[33])\n",
    "    #print(test_df.columns[66])\n",
    "    #test_df.drop(test_df.columns[[0,1,2,33,34,64,65]], axis=1,inplace=True)\n",
    "    #test_df.rename(columns={\"3_label\":\"label\"},inplace=True)\n",
    "    #print(test_df.columns)\n",
    "    #print(test_df.dtypes)\n",
    "    #a = test_df.head(5)\n",
    "    #print(a)\n",
    "    #print(test_df.columns)\n",
    "    print(len(test_df.columns))\n",
    "    \n",
    "    break\n",
    "    #test_df.to_csv('abcd.csv',header=False, mode='a',index=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['u_b1_count_x', 'u_b2_count_x', 'u_b3_count_x', 'u_b4_count_x',\n",
      "       'u_b_count_x', 'u_b1_count_y', 'u_b2_count_y', 'u_b3_count_y',\n",
      "       'u_b4_count_y', 'u_b_count_y', 'u_b1_count', 'u_b2_count', 'u_b3_count',\n",
      "       'u_b4_count', 'u_b_count', 'i_b1_count_x', 'i_b2_count_x',\n",
      "       'i_b3_count_x', 'i_b4_count_x', 'i_b_count_x', 'i_b4_rate_x',\n",
      "       'i_user_num_x', 'i_b1_count_y', 'i_b2_count_y', 'i_b3_count_y',\n",
      "       'i_b4_count_y', 'i_b_count_y', 'i_b4_rate_y', 'i_user_num_y',\n",
      "       'i_b1_count', 'i_b2_count', 'i_b3_count', 'i_b4_count', 'i_b_count',\n",
      "       'i_b4_rate', 'i_user_num', 'c_b1_count_x', 'c_b2_count_x',\n",
      "       'c_b3_count_x', 'c_b4_count_x', 'c_b_count_x', 'c_b4_rate_x',\n",
      "       'c_user_num_x', 'c_b1_count_y', 'c_b2_count_y', 'c_b3_count_y',\n",
      "       'c_b4_count_y', 'c_b_count_y', 'c_b4_rate_y', 'c_user_num_y',\n",
      "       'c_b1_count', 'c_b2_count', 'c_b3_count', 'c_b4_count', 'c_b_count',\n",
      "       'c_b4_rate', 'c_user_num', 'ic_sales_num_x', 'ic_user_num_x',\n",
      "       'ic_behavior_num_x', 'ic_sales_num_y', 'ic_user_num_y',\n",
      "       'ic_behavior_num_y', 'ic_sales_num', 'ic_user_num', 'ic_behavior_num',\n",
      "       'ui_sort_x', 'ui_b1_count_x', 'ui_b2_count_x', 'ui_b3_count_x',\n",
      "       'ui_b4_count_x', 'ui_cumcount_x', 'ui_b_count_x', 'ui_sort_y',\n",
      "       'ui_b1_count_y', 'ui_b2_count_y', 'ui_b3_count_y', 'ui_b4_count_y',\n",
      "       'ui_cumcount_y', 'ui_b_count_y', 'ui_sort', 'ui_b1_count',\n",
      "       'ui_b2_count', 'ui_b3_count', 'ui_b4_count', 'ui_cumcount',\n",
      "       'ui_b_count', 'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "a = pd.read_csv(\"../DataSet/part_data_uiclfk2014-12-05.csv\")\n",
    "b = pd.read_csv(\"../DataSet/part_data_uiclfk2014-11-21.csv\")\n",
    "c = pd.read_csv(\"../DataSet/part_data_uiclfk2014-11-28.csv\")\n",
    "all_data = pd.concat([a, b, c])\n",
    "print(all_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv(\"../DataSet/train_data_all.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   u_b1_count_x  u_b2_count_x  u_b3_count_x  u_b4_count_x  u_b_count_x  \\\n",
      "0          2247            31            91             9         2378   \n",
      "1          2115            85            77            10         2287   \n",
      "2          2115            85            77            10         2287   \n",
      "3          2332             5            95            18         2450   \n",
      "4          2045            22            74            10         2151   \n",
      "\n",
      "   u_b1_count_y  u_b2_count_y  u_b3_count_y  u_b4_count_y  u_b_count_y  ...    \\\n",
      "0          2247            31            91             9         2378  ...     \n",
      "1          2115            85            77            10         2287  ...     \n",
      "2          2115            85            77            10         2287  ...     \n",
      "3          2332             5            95            18         2450  ...     \n",
      "4          2045            22            74            10         2151  ...     \n",
      "\n",
      "   ui_cumcount_y  ui_b_count_y  ui_sort  ui_b1_count  ui_b2_count  \\\n",
      "0              2             2      432            2            0   \n",
      "1              2             2      309            2            0   \n",
      "2              2             2      567            2            0   \n",
      "3              1             1      492            1            0   \n",
      "4              1             1      492            1            0   \n",
      "\n",
      "   ui_b3_count  ui_b4_count  ui_cumcount  ui_b_count  label  \n",
      "0            0            0            2           2      0  \n",
      "1            0            0            2           2      0  \n",
      "2            0            0            2           2      0  \n",
      "3            0            0            1           1      0  \n",
      "4            0            0            1           1      0  \n",
      "\n",
      "[5 rows x 88 columns]\n",
      "Index(['u_b1_count_x', 'u_b2_count_x', 'u_b3_count_x', 'u_b4_count_x',\n",
      "       'u_b_count_x', 'u_b1_count_y', 'u_b2_count_y', 'u_b3_count_y',\n",
      "       'u_b4_count_y', 'u_b_count_y', 'u_b1_count', 'u_b2_count', 'u_b3_count',\n",
      "       'u_b4_count', 'u_b_count', 'i_b1_count_x', 'i_b2_count_x',\n",
      "       'i_b3_count_x', 'i_b4_count_x', 'i_b_count_x', 'i_b4_rate_x',\n",
      "       'i_user_num_x', 'i_b1_count_y', 'i_b2_count_y', 'i_b3_count_y',\n",
      "       'i_b4_count_y', 'i_b_count_y', 'i_b4_rate_y', 'i_user_num_y',\n",
      "       'i_b1_count', 'i_b2_count', 'i_b3_count', 'i_b4_count', 'i_b_count',\n",
      "       'i_b4_rate', 'i_user_num', 'c_b1_count_x', 'c_b2_count_x',\n",
      "       'c_b3_count_x', 'c_b4_count_x', 'c_b_count_x', 'c_b4_rate_x',\n",
      "       'c_user_num_x', 'c_b1_count_y', 'c_b2_count_y', 'c_b3_count_y',\n",
      "       'c_b4_count_y', 'c_b_count_y', 'c_b4_rate_y', 'c_user_num_y',\n",
      "       'c_b1_count', 'c_b2_count', 'c_b3_count', 'c_b4_count', 'c_b_count',\n",
      "       'c_b4_rate', 'c_user_num', 'ic_sales_num_x', 'ic_user_num_x',\n",
      "       'ic_behavior_num_x', 'ic_sales_num_y', 'ic_user_num_y',\n",
      "       'ic_behavior_num_y', 'ic_sales_num', 'ic_user_num', 'ic_behavior_num',\n",
      "       'ui_sort_x', 'ui_b1_count_x', 'ui_b2_count_x', 'ui_b3_count_x',\n",
      "       'ui_b4_count_x', 'ui_cumcount_x', 'ui_b_count_x', 'ui_sort_y',\n",
      "       'ui_b1_count_y', 'ui_b2_count_y', 'ui_b3_count_y', 'ui_b4_count_y',\n",
      "       'ui_cumcount_y', 'ui_b_count_y', 'ui_sort', 'ui_b1_count',\n",
      "       'ui_b2_count', 'ui_b3_count', 'ui_b4_count', 'ui_cumcount',\n",
      "       'ui_b_count', 'label'],\n",
      "      dtype='object')\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for test_df in pd.read_csv(\"../DataSet/part_data_uiclfk2014-12-05.csv\", chunksize = 100000):  \n",
    "    print(test_df.head(5))\n",
    "    print(test_df.columns)\n",
    "    #print(test_df.columns[2])\n",
    "    #print(test_df.columns[33])\n",
    "    #print(test_df.columns[66])\n",
    "    #test_df.drop(test_df.columns[[0,1,2,33,34,64,65]], axis=1,inplace=True)\n",
    "    #test_df.rename(columns={\"3_label\":\"label\"},inplace=True)\n",
    "    #print(test_df.columns)\n",
    "    #print(test_df.dtypes)\n",
    "    #a = test_df.head(5)\n",
    "    #print(a)\n",
    "    #print(test_df.columns)\n",
    "    print(len(test_df.columns))\n",
    "    test_df.to_csv(\"test.csv\", header=False, mode='a', index=False)  #有 bug。\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

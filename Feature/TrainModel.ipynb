{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#train data 是一部分一部分train还是之前就计算好，一次性进行训练呢？\n",
    "#现在是实现方式    打算    是按照“一次性”的方式进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data_filename = \"../DataSet/KMeans/\"+\"train_data_all\"+\".csv\"\n",
    "train_data_set = pd.read_csv(train_data_filename)\n",
    "train_data_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names=[\n",
    "        'u_b1_count_x','u_b2_count_x','u_b3_count_x','u_b4_count_x','u_b_count_x','u_b4_rate_x','u_b4_diff_hours_x',\n",
    "        'u_b1_count_y','u_b2_count_y','u_b3_count_y','u_b4_count_y','u_b_count_y','u_b4_rate_y','u_b4_diff_hours_y',\n",
    "        'u_b1_count','u_b2_count','u_b3_count','u_b4_count','u_b_count','u_b4_rate','u_b4_diff_hours',\n",
    "        'i_b1_count_x','i_b2_count_x','i_b3_count_x','i_b4_count_x','i_b_count_x','i_b4_rate_x','i_user_num_x','i_b4_diff_hours_x',\n",
    "        'i_b1_count_y','i_b2_count_y','i_b3_count_y','i_b4_count_y','i_b_count_y','i_b4_rate_y','i_user_num_y','i_b4_diff_hours_y',\n",
    "        'i_b1_count','i_b2_count','i_b3_count','i_b4_count','i_b_count','i_b4_rate','i_user_num','i_b4_diff_hours',\n",
    "        'c_b1_count_x','c_b2_count_x','c_b3_count_x','c_b4_count_x','c_b_count_x','c_b4_rate_x','c_user_num_x','c_b4_diff_hours_x',\n",
    "        'c_b1_count_y','c_b2_count_y','c_b3_count_y','c_b4_count_y','c_b_count_y','c_b4_rate_y','c_user_num_y','c_b4_diff_hours_y',                                                                              \n",
    "        'c_b1_count','c_b2_count','c_b3_count','c_b4_count','c_b_count','c_b4_rate','c_user_num','c_b4_diff_hours',\n",
    "        'ic_u_rank_in_c_x','ic_b_rank_in_c_x','ic_b4_rank_in_c_x',\n",
    "        'ic_u_rank_in_c_y','ic_b_rank_in_c_y','ic_b4_rank_in_c_y',\n",
    "        'ic_u_rank_in_c','ic_b_rank_in_c','ic_b4_rank_in_c',\n",
    "        'ui_b1_count_x','ui_b2_count_x','ui_b3_count_x','ui_b4_count_x','ui_b_count_x','ui_b_count_rank_in_u_x','ui_b_count_rank_in_uc_x',\n",
    "        'ui_b1_last_hours_x','ui_b2_last_hours_x','ui_b3_last_hours_x','ui_b4_last_hours_x',\n",
    "        'ui_b1_count_y','ui_b2_count_y','ui_b3_count_y','ui_b4_count_y','ui_b_count_y','ui_b_count_rank_in_u_y','ui_b_count_rank_in_uc_y',\n",
    "        'ui_b1_last_hours_y','ui_b2_last_hours_y','ui_b3_last_hours_y','ui_b4_last_hours_y',\n",
    "        'ui_b1_count','ui_b2_count','ui_b3_count','ui_b4_count','ui_b_count','ui_b_count_rank_in_u','ui_b_count_rank_in_uc',\n",
    "        'ui_b1_last_hours','ui_b2_last_hours','ui_b3_last_hours','ui_b4_last_hours',\n",
    "        'uc_b1_count_x','uc_b2_count_x','uc_b3_count_x','uc_b4_count_x','uc_b_count_x','uc_b_count_rank_in_u_x','uc_b1_last_hours_x','uc_b2_last_hours_x','uc_b3_last_hours_x','uc_b4_last_hours_x',\n",
    "        'uc_b1_count_y','uc_b2_count_y','uc_b3_count_y','uc_b4_count_y','uc_b_count_y','uc_b_count_rank_in_u_y','uc_b1_last_hours_y','uc_b2_last_hours_y','uc_b3_last_hours_x','uc_b4_last_hours_y',\n",
    "        'uc_b1_count','uc_b2_count','uc_b3_count','uc_b4_count','uc_b_count','uc_b_count_rank_in_u','uc_b1_last_hours','uc_b2_last_hours','uc_b3_last_hours','uc_b4_last_hours',\n",
    "        'label']\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data_filename, algorithm):\n",
    "    print('start') \n",
    "    #output file name\n",
    "    model_save_path = \"../ModelFile/\"\n",
    "    \n",
    "    train_data_set = pd.read_csv(train_data_filename,index_col = False, names=[\n",
    "        'u_b1_count_x','u_b2_count_x','u_b3_count_x','u_b4_count_x','u_b_count_x','u_b4_rate_x','u_b4_diff_hours_x',\n",
    "        'u_b1_count_y','u_b2_count_y','u_b3_count_y','u_b4_count_y','u_b_count_y','u_b4_rate_y','u_b4_diff_hours_y',\n",
    "        'u_b1_count','u_b2_count','u_b3_count','u_b4_count','u_b_count','u_b4_rate','u_b4_diff_hours',\n",
    "        'i_b1_count_x','i_b2_count_x','i_b3_count_x','i_b4_count_x','i_b_count_x','i_b4_rate_x','i_user_num_x','i_b4_diff_hours_x',\n",
    "        'i_b1_count_y','i_b2_count_y','i_b3_count_y','i_b4_count_y','i_b_count_y','i_b4_rate_y','i_user_num_y','i_b4_diff_hours_y',\n",
    "        'i_b1_count','i_b2_count','i_b3_count','i_b4_count','i_b_count','i_b4_rate','i_user_num','i_b4_diff_hours',\n",
    "        'c_b1_count_x','c_b2_count_x','c_b3_count_x','c_b4_count_x','c_b_count_x','c_b4_rate_x','c_user_num_x','c_b4_diff_hours_x',\n",
    "        'c_b1_count_y','c_b2_count_y','c_b3_count_y','c_b4_count_y','c_b_count_y','c_b4_rate_y','c_user_num_y','c_b4_diff_hours_y',                                                                              \n",
    "        'c_b1_count','c_b2_count','c_b3_count','c_b4_count','c_b_count','c_b4_rate','c_user_num','c_b4_diff_hours',\n",
    "        'ic_u_rank_in_c_x','ic_b_rank_in_c_x','ic_b4_rank_in_c_x',\n",
    "        'ic_u_rank_in_c_y','ic_b_rank_in_c_y','ic_b4_rank_in_c_y',\n",
    "        'ic_u_rank_in_c','ic_b_rank_in_c','ic_b4_rank_in_c',\n",
    "        'ui_b1_count_x','ui_b2_count_x','ui_b3_count_x','ui_b4_count_x','ui_b_count_x','ui_b_count_rank_in_u_x','ui_b_count_rank_in_uc_x',\n",
    "        'ui_b1_last_hours_x','ui_b2_last_hours_x','ui_b3_last_hours_x','ui_b4_last_hours_x',\n",
    "        'ui_b1_count_y','ui_b2_count_y','ui_b3_count_y','ui_b4_count_y','ui_b_count_y','ui_b_count_rank_in_u_y','ui_b_count_rank_in_uc_y',\n",
    "        'ui_b1_last_hours_y','ui_b2_last_hours_y','ui_b3_last_hours_y','ui_b4_last_hours_y',\n",
    "        'ui_b1_count','ui_b2_count','ui_b3_count','ui_b4_count','ui_b_count','ui_b_count_rank_in_u','ui_b_count_rank_in_uc',\n",
    "        'ui_b1_last_hours','ui_b2_last_hours','ui_b3_last_hours','ui_b4_last_hours',\n",
    "        'uc_b1_count_x','uc_b2_count_x','uc_b3_count_x','uc_b4_count_x','uc_b_count_x','uc_b_count_rank_in_u_x','uc_b1_last_hours_x','uc_b2_last_hours_x','uc_b3_last_hours_x','uc_b4_last_hours_x',\n",
    "        'uc_b1_count_y','uc_b2_count_y','uc_b3_count_y','uc_b4_count_y','uc_b_count_y','uc_b_count_rank_in_u_y','uc_b1_last_hours_y','uc_b2_last_hours_y','uc_b3_last_hours_x','uc_b4_last_hours_y',\n",
    "        'uc_b1_count','uc_b2_count','uc_b3_count','uc_b4_count','uc_b_count','uc_b_count_rank_in_u','uc_b1_last_hours','uc_b2_last_hours','uc_b3_last_hours','uc_b4_last_hours',\n",
    "        'label'])  \n",
    "    print(\"train_data_set \\n\",train_data_set.head(5),train_data_set.shape)\n",
    "    #print(\"train_data_set\",train_data_set.columns)\n",
    "    #print(\"train_data_set\",train_data_set.shape)\n",
    "    #sample part\n",
    "    train_data_set_buy = train_data_set[train_data_set.label == 1]\n",
    "    train_data_set_not_buy = train_data_set[train_data_set.label == 0]\n",
    "    #print(\"train set shape-buy:\",train_data_set_buy.shape)\n",
    "    #print(\"train set shape-not buy:\",train_data_set_not_buy.shape)\n",
    "    #print(\"N\\P ratio is:\",train_data_set_not_buy.shape[0]/train_data_set_buy.shape[0])\n",
    "    #train_data_set_not_buy = train_data_set_not_buy.sample(frac=0.05, replace=True, random_state=1)\n",
    "    #print(\"train set shape-not buy after sample:\",train_data_set_not_buy.shape)\n",
    "    #print(\"after sample N\\P ratio is:\",train_data_set_not_buy.shape[0]/train_data_set_buy.shape[0])\n",
    "    #train_data_set_sample =  train_data_set_not_buy.append(train_data_set_buy)\n",
    "    #print(\"train set shape:\",train_data_set_sample.shape)\n",
    "    \n",
    "    train_data_set_temp = train_data_set.iloc[:,:-1]  \n",
    "    #print(\"train_data_set_temp\",train_data_set_temp.columns)\n",
    "    #print(\"train_data_set_temp\",train_data_set_temp.shape)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(train_data_set_temp.values)\n",
    "    train_data_set_temp_values = scaler.transform(train_data_set_temp.values)\n",
    "    train_data_set_scale = pd.DataFrame(train_data_set_temp_values)\n",
    "    #train_data_set_sample.reset_index(inplace=True)        #需要reset_index 吗？会不会导致错误？\n",
    "    train_data_set_scale = train_data_set_scale.join(train_data_set['label'])\n",
    "    #print(\"train_data_set_scale\",train_data_set_scale.head(5))\n",
    "    #print(\"train_data_set_scale\",train_data_set_scale.columns)\n",
    "    #print(\"train_data_set_scale\",train_data_set_scale.shape)\n",
    "    #cross validation\n",
    "    #others\n",
    "    df_train_data_X = train_data_set_scale.iloc[:,:-1]\n",
    "    df_train_data_y = train_data_set_scale.loc[:,['label']]\n",
    " \n",
    "    if algorithm == 'LR':\n",
    "        print('LR:')\n",
    "        save_path_name=model_save_path+\"lr_\"+\"train_model.m\"\n",
    "        train_model_start_time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print('train model start time:',train_model_start_time)\n",
    "        modle_lr = LogisticRegression(penalty='l1',solver='liblinear',multi_class='ovr').fit(df_train_data_X, df_train_data_y)\n",
    "        train_model_finish_time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print('train model finish time:',train_model_finish_time)\n",
    "        print(modle_lr)      \n",
    "        print(\"LR model save...\")\n",
    "        joblib.dump(modle_lr, save_path_name)\n",
    "        #modle_lr = joblib.load(save_path_name)\n",
    "    elif algorithm == 'RF':\n",
    "        print('RF')\n",
    "        save_path_name=model_save_path+\"RF_\"+\"train_model.m\"\n",
    "        train_model_start_time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print('train model start time:',train_model_start_time)\n",
    "        modle_rf = RandomForestClassifier(n_estimators=200, max_depth=10,n_jobs=-1).fit(df_train_data_X, df_train_data_y)\n",
    "        train_model_finish_time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print('train model finish time:',train_model_finish_time)\n",
    "        print(modle_rf)      \n",
    "        print(\"RF model save...\")\n",
    "        joblib.dump(modle_rf, save_path_name)\n",
    "        \n",
    "        #res = modle_rf.predict(df_part_4_predict_norm.values)\n",
    "        \n",
    "        #print(modle_rf)\n",
    "    elif algorithm == 'GBDT':\n",
    "        print('GBDT')\n",
    "        save_path_name=model_save_path+\"GBDT_\"+\"train_model.m\"\n",
    "        train_model_start_time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print('train model start time:',train_model_start_time)\n",
    "        #modle_gbdt = GradientBoostingClassifier(n_estimators=500,validation_fraction=0.2,n_iter_no_change=5, tol=0.01,random_state=0).fit(df_train_data_X, df_train_data_y)\n",
    "        modle_gbdt = GradientBoostingClassifier(max_depth=12, \n",
    "                                                                  min_samples_leaf=10, \n",
    "                                                                  learning_rate=0.05, \n",
    "                                                                  n_estimators=180,  \n",
    "                                                                  subsample=0.8, \n",
    "                                                                  max_features=\"sqrt\",\n",
    "                                                                  verbose=1,\n",
    "                                                                  n_iter_no_change=None,\n",
    "                                                                  tol=1e-4).fit(df_train_data_X, df_train_data_y)\n",
    "        print(\"update parameter\")\n",
    "        \n",
    "        train_model_finish_time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print('train model finish time:',train_model_finish_time)\n",
    "        print(modle_gbdt)      \n",
    "        print(\"GBDT model save...\")\n",
    "        joblib.dump(modle_gbdt, save_path_name)\n",
    "        #res = modle_gbdt.predict(df_part_4_predict_norm.values)\n",
    "    elif algorithm == 'AdaBoost':\n",
    "        print('AdaBoost')\n",
    "        modle_adaboost =AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm=\"SAMME\",n_estimators=200).fit(df_train_data_X, df_train_data_y)\n",
    "        res = modle_adaboost.predict(df_part_4_predict_norm.values)\n",
    "    elif algorithm == 'XGBoost':\n",
    "        print('XGBoost')\n",
    "        print('sklearn中无此算法')\n",
    "        #modle_xgboost = LogisticRegression(penalty='l1',solver='liblinear',multi_class='ovr').fit(df_train_data_X, df_train_data_y)\n",
    "        #res = modle_xgboost.predict(df_part_4_predict_norm.values)\n",
    "    elif algorithm == 'FM':\n",
    "        print('FM')\n",
    "        print('sklean中无此算法')\n",
    "        #modle_fm = LogisticRegression(penalty='l1',solver='liblinear',multi_class='ovr').fit(df_train_data_X, df_train_data_y)\n",
    "        #res = modle_fm.predict(df_part_4_predict_norm.values)\n",
    "    elif algorithm =='SVM':\n",
    "        print('SVM')\n",
    "        modle_svm = SVC(gamma='auto').fit(df_train_data_X, df_train_data_y)\n",
    "        res = modle_svm.predict(df_part_4_predict_norm.values)\n",
    "    else:\n",
    "        print('wrong algorithm name')\n",
    "    print('train model finished')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Unit Test As Follows："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data_filename = \"../DataSet/KMeans/\"+\"train_data_all\"+\".csv\"\n",
    "train_model(train_data_filename, 'GBDT')      #train_data_filename wait to add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

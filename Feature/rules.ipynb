{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCertainDayCertainBehavior(user_behavior_data_path,start_day,end_day,bt):\n",
    "    second_end_day = (end_day - datetime.timedelta(days=1))    \n",
    "    start_day_str = str(start_day.date())\n",
    "    end_day_str = str(end_day.date())\n",
    "    print(\"end day:\",end_day_str)\n",
    "    second_end_day_str = str(second_end_day.date())\n",
    "    #output file name\n",
    "    part_data_file_name = \"../DataSet/Rules/\"+\"part_data\"+end_day_str+\"b\"+str(bt)+\".csv\"\n",
    "    \n",
    "    item_table = pd.read_csv('../DataSet/tianchi_fresh_comp_train_item.csv')\n",
    "    #predict_data_set_buy = predict_data_set_buy[predict_data_set_buy.item_id.isin(list(item_table.item_id))]\n",
    "    #predict_data_set_buy[['user_id','item_id']].to_csv(\"../DataSet/tianchi_mobile_recommendation_predict.csv\",index=None)\n",
    "    print(\"read sub set end\")\n",
    "        \n",
    "    batch = 0\n",
    "    dateparse = lambda dates: pd.datetime.strptime(dates, '%Y-%m-%d %H')\n",
    "    for df in pd.read_csv(open(user_behavior_data_path, 'r'), \n",
    "                      parse_dates=['time'], \n",
    "                      index_col = ['time'], \n",
    "                      date_parser = dateparse,\n",
    "                      chunksize = 10000000):  # operation on chunk as the data file is too large\n",
    "        try:\n",
    "            part_data = df[end_day_str]\n",
    "            part_data = part_data[part_data.behavior_type==bt]\n",
    "            part_data = part_data[part_data.item_id.isin(list(item_table.item_id))] #included in sub set\n",
    "            part_data.to_csv(part_data_file_name,  \n",
    "                             columns=['user_id','item_id','behavior_type'],\n",
    "                             header=False, mode='a')              \n",
    "            \n",
    "            batch += 1\n",
    "            print('chunk %d done.' %batch) \n",
    "        except StopIteration:\n",
    "            print(\"divide the data set finish.\")\n",
    "            break \n",
    "    print(\"end\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"test = pd.read_csv('../DataSet/DevideData/part_data2014-12-09to2014-12-14.csv')\\ntest.columns = ['time','user_id','item_id','behavior_type','test']\\ntest[test.time>'2014-12-14']\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''test = pd.read_csv('../DataSet/DevideData/part_data2014-12-09to2014-12-14.csv')\n",
    "test.columns = ['time','user_id','item_id','behavior_type','test']\n",
    "test[test.time>'2014-12-14']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doNotBuyUser(user_behavior_data_path,start_day,end_day):\n",
    "    second_end_day = (end_day - datetime.timedelta(days=1))    \n",
    "    start_day_str = str(start_day.date())\n",
    "    end_day_str = str(end_day.date())\n",
    "    print(\"end day:\",end_day_str)\n",
    "    second_end_day_str = str(second_end_day.date())\n",
    "    print(\"second_end_day:\",second_end_day)\n",
    "    name = start_day_str + \"to\" + second_end_day_str\n",
    "    #output file name\n",
    "    part_data_file_name = \"../DataSet/Rules/\"+\"notbuyusers\"+name+\".csv\"\n",
    "    \n",
    "    item_table = pd.read_csv('../DataSet/tianchi_fresh_comp_train_item.csv')\n",
    "    #predict_data_set_buy = predict_data_set_buy[predict_data_set_buy.item_id.isin(list(item_table.item_id))]\n",
    "    #predict_data_set_buy[['user_id','item_id']].to_csv(\"../DataSet/tianchi_mobile_recommendation_predict.csv\",index=None)\n",
    "    print(\"read sub set end\")\n",
    "        \n",
    "    batch = 0\n",
    "    dateparse = lambda dates: pd.datetime.strptime(dates, '%Y-%m-%d %H')\n",
    "    for df in pd.read_csv(open(user_behavior_data_path, 'r'), \n",
    "                      parse_dates=['time'], \n",
    "                      index_col = ['time'], \n",
    "                      date_parser = dateparse,\n",
    "                      chunksize = 10000000):  # operation on chunk as the data file is too large\n",
    "        try:\n",
    "            part_data = df[start_day_str:second_end_day_str]  #has problem\n",
    "            part_data_buy = part_data[part_data.behavior_type==4]\n",
    "            part_data_buy = part_data_buy.drop_duplicates(['user_id'])\n",
    "'''            part_data_3 = part_data[part_data.behavior_type==3]\n",
    "            part_data_2 = part_data[part_data.behavior_type==2]\n",
    "            part_data_2 = part_data[part_data.behavior_type==2]'''\n",
    "            part_data = part_data[~part_data.user_id.isin(list(part_data_buy.user_id))] #included in sub set\n",
    "            part_data = part_data.drop_duplicates(['user_id'])\n",
    "            part_data.to_csv(part_data_file_name,  \n",
    "                             columns=['user_id','item_id','behavior_type'],\n",
    "                             header=False, mode='a')              \n",
    "            \n",
    "            batch += 1\n",
    "            print('chunk %d done.' %batch) \n",
    "        except StopIteration:\n",
    "            print(\"divide the data set finish.\")\n",
    "            break \n",
    "    print(\"end\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end day: 2014-12-17\n",
      "second_end_day: 2014-12-16 00:00:00\n",
      "read sub set end\n",
      "chunk 1 done.\n",
      "chunk 2 done.\n",
      "chunk 3 done.\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "'''start = datetime.datetime(2014,12,3,0,0,0)\n",
    "end = datetime.datetime(2014,12,17,0,0,0)\n",
    "doNotBuyUser(\"../DataSet/tianchi_fresh_comp_train_user.csv\", start, end)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (781, 4)\n",
      "part_data:\n",
      "                    time_x    user_id    item_id  behavior_type_x  label  \\\n",
      "2     2014-12-17 00:00:00  100442521  244784735                3      1   \n",
      "3     2014-12-17 15:00:00  100442521  225667411                3      1   \n",
      "4     2014-12-17 00:00:00  100442521  127658419                3      1   \n",
      "5     2014-12-17 00:00:00  100442521   87030081                3      1   \n",
      "6     2014-12-17 00:00:00  100442521  127819102                3      1   \n",
      "7     2014-12-17 00:00:00  100442521  362002225                3      1   \n",
      "8     2014-12-17 13:00:00  100442521  333445454                3      1   \n",
      "9     2014-12-17 07:00:00  101105140   16585103                3      1   \n",
      "14    2014-12-17 19:00:00  103424518  301444118                3      1   \n",
      "15    2014-12-17 22:00:00  103490614  226466959                3      1   \n",
      "16    2014-12-17 14:00:00  104428489  120341900                3      1   \n",
      "18    2014-12-17 12:00:00  104982265  277789830                3      1   \n",
      "21    2014-12-17 23:00:00  105410696  208669010                3      1   \n",
      "22    2014-12-17 23:00:00  105410696   81919234                3      1   \n",
      "23    2014-12-17 23:00:00  105792755  382358582                3      1   \n",
      "25    2014-12-17 20:00:00  106579874   96684394                3      1   \n",
      "27    2014-12-17 20:00:00  108324380  107852873                3      1   \n",
      "51    2014-12-17 16:00:00  110824750  358722916                3      1   \n",
      "53    2014-12-17 17:00:00  111246442  296365885                3      1   \n",
      "54    2014-12-17 18:00:00  111246442  203047878                3      1   \n",
      "55    2014-12-17 18:00:00  111246442  388110765                3      1   \n",
      "56    2014-12-17 17:00:00  111246442  271420242                3      1   \n",
      "58    2014-12-17 22:00:00  113743271   23847066                3      1   \n",
      "59    2014-12-17 22:00:00  113743271   77075490                3      1   \n",
      "91    2014-12-17 21:00:00  114475283  226466959                3      1   \n",
      "92    2014-12-17 21:00:00  114475283  241588722                3      1   \n",
      "98    2014-12-17 11:00:00  115761083    1033858                3      1   \n",
      "99    2014-12-17 11:00:00  115761083  204139899                3      1   \n",
      "115   2014-12-17 23:00:00  116270315  274415103                3      1   \n",
      "116   2014-12-17 10:00:00  116592686   49002319                3      1   \n",
      "...                   ...        ...        ...              ...    ...   \n",
      "1767  2014-12-17 19:00:00   56782266   21226919                3      1   \n",
      "1768  2014-12-17 21:00:00   56782266   31985414                3      1   \n",
      "1769  2014-12-17 21:00:00   56782266  104661234                3      1   \n",
      "1770  2014-12-17 21:00:00   56782266  318484237                3      1   \n",
      "1771  2014-12-17 00:00:00   56850825   19749520                3      1   \n",
      "1774  2014-12-17 12:00:00   57476829  308721717                3      1   \n",
      "1775  2014-12-17 22:00:00   58308738  164971070                3      1   \n",
      "1776  2014-12-17 16:00:00   58891608  112135226                3      1   \n",
      "1777  2014-12-17 16:00:00   58891608  197176044                3      1   \n",
      "1778  2014-12-17 16:00:00   58891608  279320012                3      1   \n",
      "1779  2014-12-17 19:00:00   59318280  144505863                3      1   \n",
      "1780  2014-12-17 19:00:00   60910328  204485346                3      1   \n",
      "1781  2014-12-17 14:00:00   60910328  357838269                3      1   \n",
      "1782  2014-12-17 21:00:00   61607522  403004154                3      1   \n",
      "1798  2014-12-17 14:00:00   62844941   47909770                3      1   \n",
      "1799  2014-12-17 23:00:00   62844941  367516265                3      1   \n",
      "1800  2014-12-17 15:00:00   64014341   84377003                3      1   \n",
      "1801  2014-12-17 23:00:00   64151780   50983705                3      1   \n",
      "1802  2014-12-17 23:00:00   64151780  190774778                3      1   \n",
      "1803  2014-12-17 23:00:00   64151780  100176494                3      1   \n",
      "1804  2014-12-17 23:00:00   64151780   37990904                3      1   \n",
      "1805  2014-12-17 22:00:00   64151780  255103751                3      1   \n",
      "1806  2014-12-17 10:00:00   64163546  306955836                3      1   \n",
      "1807  2014-12-17 11:00:00   64163546  110679917                3      1   \n",
      "1808  2014-12-17 11:00:00   64163546  277020293                3      1   \n",
      "1809  2014-12-17 10:00:00   64163546  108589584                3      1   \n",
      "1810  2014-12-17 11:00:00   64163546  319888585                3      1   \n",
      "1811  2014-12-17 11:00:00   64163546   93707622                3      1   \n",
      "1812  2014-12-17 11:00:00   64163546   89883341                3      1   \n",
      "1813  2014-12-17 10:00:00   64163546  219634967                3      1   \n",
      "\n",
      "     time_y  behavior_type_y  \n",
      "2        -1             -1.0  \n",
      "3        -1             -1.0  \n",
      "4        -1             -1.0  \n",
      "5        -1             -1.0  \n",
      "6        -1             -1.0  \n",
      "7        -1             -1.0  \n",
      "8        -1             -1.0  \n",
      "9        -1             -1.0  \n",
      "14       -1             -1.0  \n",
      "15       -1             -1.0  \n",
      "16       -1             -1.0  \n",
      "18       -1             -1.0  \n",
      "21       -1             -1.0  \n",
      "22       -1             -1.0  \n",
      "23       -1             -1.0  \n",
      "25       -1             -1.0  \n",
      "27       -1             -1.0  \n",
      "51       -1             -1.0  \n",
      "53       -1             -1.0  \n",
      "54       -1             -1.0  \n",
      "55       -1             -1.0  \n",
      "56       -1             -1.0  \n",
      "58       -1             -1.0  \n",
      "59       -1             -1.0  \n",
      "91       -1             -1.0  \n",
      "92       -1             -1.0  \n",
      "98       -1             -1.0  \n",
      "99       -1             -1.0  \n",
      "115      -1             -1.0  \n",
      "116      -1             -1.0  \n",
      "...     ...              ...  \n",
      "1767     -1             -1.0  \n",
      "1768     -1             -1.0  \n",
      "1769     -1             -1.0  \n",
      "1770     -1             -1.0  \n",
      "1771     -1             -1.0  \n",
      "1774     -1             -1.0  \n",
      "1775     -1             -1.0  \n",
      "1776     -1             -1.0  \n",
      "1777     -1             -1.0  \n",
      "1778     -1             -1.0  \n",
      "1779     -1             -1.0  \n",
      "1780     -1             -1.0  \n",
      "1781     -1             -1.0  \n",
      "1782     -1             -1.0  \n",
      "1798     -1             -1.0  \n",
      "1799     -1             -1.0  \n",
      "1800     -1             -1.0  \n",
      "1801     -1             -1.0  \n",
      "1802     -1             -1.0  \n",
      "1803     -1             -1.0  \n",
      "1804     -1             -1.0  \n",
      "1805     -1             -1.0  \n",
      "1806     -1             -1.0  \n",
      "1807     -1             -1.0  \n",
      "1808     -1             -1.0  \n",
      "1809     -1             -1.0  \n",
      "1810     -1             -1.0  \n",
      "1811     -1             -1.0  \n",
      "1812     -1             -1.0  \n",
      "1813     -1             -1.0  \n",
      "\n",
      "[877 rows x 7 columns]\n",
      "reference_data_set_buy: 781\n",
      "predict_data_set_buy: 877\n",
      "right_res: 39\n",
      "P: 0.04446978335233751 \n",
      " R: 0.0499359795134443 \n",
      " F1: 0.047044632086851626\n"
     ]
    }
   ],
   "source": [
    "#rule1\n",
    "part_data = pd.read_csv(\"../DataSet/Rules/part_data2014-12-17b3.csv\", header=None, index_col=False)\n",
    "part_data.columns = ['time','user_id','item_id','behavior_type']\n",
    "part_data.drop_duplicates(['user_id','item_id'])\n",
    "part_data['label'] = 1\n",
    "#rule2\n",
    "part_data_test = pd.read_csv(\"../DataSet/Rules/part_data2014-12-18b4.csv\", header=None, index_col=False)\n",
    "part_data_test.columns = ['time','user_id','item_id','behavior_type']\n",
    "print('shape:',part_data_test.shape)\n",
    "part_data_test.drop_duplicates(['user_id','item_id'])\n",
    "part_data_test['label']=1\n",
    "part_data_test.head(5)\n",
    "\n",
    "part_data_b4 = pd.read_csv(\"../DataSet/Rules/part_data2014-12-17b4.csv\", header=None, index_col=False)\n",
    "part_data_b4.columns = ['time','user_id','item_id','behavior_type']\n",
    "remove_from_b3 = part_data.merge(part_data_b4,on=['item_id','user_id'],how='left').fillna(-1)\n",
    "part_data = remove_from_b3[remove_from_b3.behavior_type_y==-1]\n",
    "#rule3:先买了，在加购物车的。事实证明先买了的再加购物车的第二天也不会买\n",
    "'''a =remove_from_b3[remove_from_b3.time_y !=-1]\n",
    "b = a[a.time_y < a.time_x]\n",
    "part_data = pd.concat([part_data, b])'''\n",
    "#rule4:当天有过购买行为的用户都去出掉\n",
    "part_data = part_data[~part_data.user_id.isin(list(part_data_b4.user_id))] #included in sub set\n",
    "\n",
    "#rule5:前半个月从买没买过东西的去处掉\n",
    "'''not_buy_users = pd.read_csv(\"../DataSet/Rules/notbuyusers2014-12-03to2014-12-16.csv\", header=None, index_col=False)\n",
    "not_buy_users.columns = ['time','user_id','item_id','behavior_type']\n",
    "part_data = part_data[~part_data.user_id.isin(list(not_buy_users.user_id))] #included in sub set\n",
    "\n",
    "print('part_data:\\n',part_data)'''\n",
    "\n",
    "predict_data_set_buy = part_data\n",
    "reference_data_set_buy = part_data_test\n",
    "right_res = predict_data_set_buy.merge(reference_data_set_buy,on=['item_id','user_id'])\n",
    "\n",
    "#caculate P, R and F1  \n",
    "R = right_res.shape[0]/reference_data_set_buy.shape[0]\n",
    "P = right_res.shape[0]/predict_data_set_buy.shape[0]\n",
    "F1 = 2*P*R/(P+R)\n",
    "print('reference_data_set_buy:',reference_data_set_buy.shape[0])\n",
    "#print('predict_data_set:',predict_data_set.shape[0])\n",
    "print('predict_data_set_buy:',predict_data_set_buy.shape[0])\n",
    "print('right_res:',right_res.shape[0])\n",
    "print('P:',P,'\\n','R:',R,'\\n','F1:',F1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1、拿12月17日加购物车的数据预测为12月18日购买的user-item对。\n",
    "reference_data_set_buy: 781\n",
    "predict_data_set_buy: 1764\n",
    "right_res: 61\n",
    "P: 0.03458049886621315 \n",
    " R: 0.07810499359795134 \n",
    " F1: 0.04793713163064833\n",
    "2、在1的基础上，将12月17日加入购物车的，同时也购买过的user-item对从预测数据集中去除\n",
    "reference_data_set_buy: 781\n",
    "predict_data_set_buy: 1415\n",
    "right_res: 59\n",
    "P: 0.04169611307420495 \n",
    " R: 0.07554417413572344 \n",
    " F1: 0.05373406193078325\n",
    "3、在2的基础上，将买了之后又加购物车的也作为预测内容。但数据证明，买了之后再加购物车的第二天也不会买了。   \n",
    "reference_data_set_buy: 781\n",
    "predict_data_set_buy: 1427\n",
    "right_res: 59\n",
    "P: 0.04134548002803083 \n",
    " R: 0.07554417413572344 \n",
    " F1: 0.05344202898550724\n",
    "4\n",
    "[1056 rows x 7 columns]\n",
    "reference_data_set_buy: 781\n",
    "predict_data_set_buy: 1056\n",
    "right_res: 52\n",
    "P: 0.04924242424242424 \n",
    " R: 0.06658130601792574 \n",
    " F1: 0.056614044637996734\n",
    "5\n",
    "877 rows x 7 columns]\n",
    "reference_data_set_buy: 781\n",
    "predict_data_set_buy: 877\n",
    "right_res: 39\n",
    "P: 0.04446978335233751 \n",
    " R: 0.0499359795134443 \n",
    " F1: 0.047044632086851626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule1(data):\n",
    "    part_data = pd.read_csv(\"part_data2014-12-17b3.csv\", header=None, index_col=False)\n",
    "    part_data.columns = [user_id','item_id','behavior_type']\n",
    "    part_data.drop_duplicates([])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read sub set end\n",
      "chunk 1 done.\n",
      "chunk 2 done.\n",
      "chunk 3 done.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime(2014,12,5,0,0,0)\n",
    "end = datetime.datetime(2014,12,18,0,0,0)\n",
    "getCertainDayCertainBehavior(\"../DataSet/tianchi_fresh_comp_train_user.csv\", start, end,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end day: 2014-12-18\n",
      "read sub set end\n",
      "chunk 1 done.\n",
      "chunk 2 done.\n",
      "chunk 3 done.\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime(2014,12,5,0,0,0)\n",
    "end = datetime.datetime(2014,12,18,0,0,0)\n",
    "getCertainDayCertainBehavior(\"../DataSet/tianchi_fresh_comp_train_user.csv\", start, end,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end day: 2014-12-17\n",
      "read sub set end\n",
      "chunk 1 done.\n",
      "chunk 2 done.\n",
      "chunk 3 done.\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime(2014,12,5,0,0,0)\n",
    "end = datetime.datetime(2014,12,17,0,0,0)\n",
    "getCertainDayCertainBehavior(\"../DataSet/tianchi_fresh_comp_train_user.csv\", start, end,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end day: 2014-12-17\n",
      "read sub set end\n",
      "chunk 1 done.\n",
      "chunk 2 done.\n",
      "chunk 3 done.\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime(2014,12,5,0,0,0)\n",
    "end = datetime.datetime(2014,12,17,0,0,0)\n",
    "getCertainDayCertainBehavior(\"../DataSet/tianchi_fresh_comp_train_user.csv\", start, end,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最终提交答案的代码\n",
    "#rule1\n",
    "part_data = pd.read_csv(\"../DataSet/Rules/part_data2014-12-18b3.csv\", header=None, index_col=False)\n",
    "part_data.columns = ['time','user_id','item_id','behavior_type']\n",
    "part_data.drop_duplicates(['user_id','item_id'])\n",
    "part_data['label'] = 1\n",
    "\n",
    "#rule2\n",
    "part_data_b4 = pd.read_csv(\"../DataSet/Rules/part_data2014-12-18b4.csv\", header=None, index_col=False)\n",
    "part_data_b4.columns = ['time','user_id','item_id','behavior_type']\n",
    "remove_from_b3 = part_data.merge(part_data_b4,on=['item_id','user_id'],how='left').fillna(-1)\n",
    "part_data = remove_from_b3[remove_from_b3.behavior_type_y==-1]\n",
    "#rule3:先买了，在加购物车的。事实证明先买了的再加购物车的第二天也不会买\n",
    "'''a =remove_from_b3[remove_from_b3.time_y !=-1]\n",
    "b = a[a.time_y < a.time_x]\n",
    "part_data = pd.concat([part_data, b])'''\n",
    "#rule4:当天有过购买行为的用户都去出掉\n",
    "part_data = part_data[~part_data.user_id.isin(list(part_data_b4.user_id))] #included in sub set\n",
    "\n",
    "#rule5:前半个月从买没买过东西的去处掉\n",
    "'''not_buy_users = pd.read_csv(\"../DataSet/Rules/notbuyusers2014-12-03to2014-12-16.csv\", header=None, index_col=False)\n",
    "not_buy_users.columns = ['time','user_id','item_id','behavior_type']\n",
    "part_data = part_data[~part_data.user_id.isin(list(not_buy_users.user_id))] #included in sub set\n",
    "\n",
    "print('part_data:\\n',part_data)'''\n",
    "item_table = pd.read_csv('../DataSet/tianchi_fresh_comp_train_item.csv')\n",
    "part_data = part_data[part_data.item_id.isin(list(item_table.item_id))] #included in sub set\n",
    "part_data[['user_id','item_id']].to_csv(\"../DataSet/Rules/tianchi_mobile_recommendation_predict.csv\",index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

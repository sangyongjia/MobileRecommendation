{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#验证数据集\n",
    "def get_reference_set(user_behavior_data,label_day):\n",
    "    #user_behavior_data_copy = user_behavior_data\n",
    "    #data_train_ui = user_behavior_data['user_id'] + user_behavior_data['item_id']\n",
    "    label_day_data = user_behavior_data[user_behavior_data['daystime'] == label_day]\n",
    "    #print(len(label_day_data))\n",
    "    data_label_buy = label_day_data[label_day_data['behavior_type'] == 4]\n",
    "    #data_label_buy_ui = data_label_buy['user_id'] + data_label_buy['item_id']\n",
    "    #data_train_labeled = data_train_ui.isin(list(data_label_buy_ui))\n",
    "    #print(\"\")\n",
    "    #dict = {True: 1, False: 0}\n",
    "    #data_train_labeled = data_train_labeled.map(dict)\n",
    "    #user_behavior_data['label'] = data_train_labeled\n",
    "    #此处的去重操作能将非购买行为去除掉（按照上边的逻辑，用户对某件商品在购买前的浏览和加购物车行为也会被标记为label==1），但同时也去除了重复购买行为\n",
    "    print('user_behavior_data shape before drop_duplicates function:',user_behavior_data.shape)\n",
    "    data_label_buy.drop_duplicates(['user_id', 'item_id'],inplace=True)\n",
    "    data_label_buy = data_label_buy[data_label_buy.item_id.isin(list(item_table.item_id))]\n",
    "    return data_label_buy[['user_id', 'item_id']]\n",
    "\n",
    "def get_label_set(user_behavior_data,label_day):\n",
    "    '''\n",
    "    label_day_data = user_behavior_data[user_behavior_data['daystime'] == label_day]\n",
    "    label_day_data_ui = label_day_data['user_id'] + label_day_data['item_id']\n",
    "    data_label_buy = label_day_data[label_day_data['behavior_type'] == 4]\n",
    "    data_label_buy_ui = data_label_buy['user_id'] + data_label_buy['item_id']\n",
    "    #print(data_label_buy_ui.dtypes)\n",
    "    data_train_labeled = label_day_data_ui.isin(list(data_label_buy_ui))\n",
    "    dict = {True: 1, False: 0}\n",
    "    data_train_labeled = data_train_labeled.map(dict)\n",
    "    label_day_data['label'] = data_train_labeled\n",
    "    #此处的去重操作能将非购买行为去除掉（按照上边的逻辑，用户对某件商品在购买前的浏览和加购物车行为也会被标记为label==1），但同时也去除了重复购买行为\n",
    "    label_day_data.drop_duplicates(['user_id', 'item_id'],inplace=True)\n",
    "    print(label_day_data.shape)\n",
    "    return label_day_data[['user_id', 'item_id','item_category', 'label']]\n",
    "    '''\n",
    "    label_day_data = user_behavior_data[user_behavior_data['daystime'] == label_day]\n",
    "    data_label_buy = label_day_data[label_day_data['behavior_type'] == 4]\n",
    "    print(label_day,\"buy behavior before drop duplicates\",data_label_buy.shape)\n",
    "    data_label_buy.drop_duplicates(['user_id', 'item_id'],inplace=True)\n",
    "    print(label_day,\"buy behavior after drop duplicates\",data_label_buy.shape)\n",
    "    print(label_day,\"buy behavior after drop duplicates, in sub product set\",data_label_buy[data_label_buy.item_id.isin(list(item_table.item_id))].shape)\n",
    "    data_label_buy['label'] = 1\n",
    "    \n",
    "    other_day_data = user_behavior_data[user_behavior_data['daystime'] != label_day]\n",
    "    other_day_data.drop_duplicates(['user_id', 'item_id'],inplace=True)\n",
    "    #print(other_day_data.head(5))\n",
    "    label_data = other_day_data.merge(data_label_buy, how='left', on=['item_id','user_id']).fillna(0)\n",
    "    #print(label_data.head(5))\n",
    "    print(label_day,\"other day data join label day buy data\",label_data[label_data['label']==1].shape)\n",
    "    '''\n",
    "    data_label_not_buy = label_day_data[label_day_data['behavior_type'] != 4]\n",
    "    print(label_day,\"not buy behavior before drop duplicates\",data_label_not_buy.shape)\n",
    "    data_label_not_buy.drop_duplicates(['user_id', 'item_id'],inplace=True)\n",
    "    print(label_day,\"not buy behavior after drop duplicates\",data_label_not_buy.shape)\n",
    "    data_label_not_buy['label'] = 0\n",
    "\n",
    "    label_day_data = data_label_not_buy.append(data_label_buy)\n",
    "    print(label_day_data.shape)\n",
    "    label_day_data.drop_duplicates(['user_id', 'item_id'], keep='last', inplace=True)\n",
    "    print(label_day_data.shape)\n",
    "    '''\n",
    "    label_data = label_data[[\"item_category_x\",\"user_id\",\"item_id\",\"label\"]]\n",
    "    #print(label_data.head(5))\n",
    "    label_data.rename(columns = {\"item_category_x\":'item_category'},inplace=True)\n",
    "    label_data.drop_duplicates(['user_id', 'item_id'],inplace=True)\n",
    "    print(label_day,\"buy behavior after join other day data ，then drop duplicate\",label_data[label_data['label']==1].shape)\n",
    "    a = label_data[label_data['label']==1]\n",
    "    a = a[a.item_id.isin(list(item_table.item_id))]\n",
    "    print(label_day,\"buy behavior after join other day data ，then drop duplicate, then in sub product set:\",a.shape)\n",
    "    return label_data[['user_id', 'item_id','item_category', 'label']]\n",
    "\n",
    "\n",
    "#user_table = pd.read_csv('../DataSet/tianchi_fresh_comp_train_user.csv')\n",
    "#item_table = pd.read_csv('../DataSet/tianchi_fresh_comp_train_item.csv')\n",
    "#user_table = user_table[user_table.item_id.isin(list(item_table.item_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nreference_data_set_buy = ReferenceData\\nreference_data_set_buy = reference_data_set_buy[reference_data_set_buy.item_id.isin(list(item_table.item_id))]\\n\\nright_res = predict_data_set_buy.merge(reference_data_set_buy,on=['item_id','user_id'])\\n#caculate P, R and F1  \\nR = right_res.shape[0]/reference_data_set_buy.shape[0]\\nP = right_res.shape[0]/predict_data_set_buy.shape[0]\\nF1 = 2*P*R/(P+R)\\nprint('reference_data_set_buy:',reference_data_set_buy.shape[0])\\nprint('predict_data_set:',predict_data_set.shape[0])\\nprint('predict_data_set_buy:',predict_data_set_buy.shape[0])\\nprint('right_res:',right_res.shape[0])\\nprint('P:',P,'\\n','R:',R,'\\n','F1:',F1)\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "reference_data_set_buy = ReferenceData\n",
    "reference_data_set_buy = reference_data_set_buy[reference_data_set_buy.item_id.isin(list(item_table.item_id))]\n",
    "\n",
    "right_res = predict_data_set_buy.merge(reference_data_set_buy,on=['item_id','user_id'])\n",
    "#caculate P, R and F1  \n",
    "R = right_res.shape[0]/reference_data_set_buy.shape[0]\n",
    "P = right_res.shape[0]/predict_data_set_buy.shape[0]\n",
    "F1 = 2*P*R/(P+R)\n",
    "print('reference_data_set_buy:',reference_data_set_buy.shape[0])\n",
    "print('predict_data_set:',predict_data_set.shape[0])\n",
    "print('predict_data_set_buy:',predict_data_set_buy.shape[0])\n",
    "print('right_res:',right_res.shape[0])\n",
    "print('P:',P,'\\n','R:',R,'\\n','F1:',F1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use_predict_proba = False  #only suit for LR now\n",
    "def predict(start_day, end_day, algorithm, use_predict_proba, validate_or_test_flag=True,prob_value=0.4):  \n",
    "    start_day_str = str(start_day.date())\n",
    "    end_day_str = str(end_day.date())\n",
    "    model_save_path = \"../ModelFile/\"\n",
    "    input_filename = \"../DataSet/Feature/\"+\"part_data_uiclf\"+end_day_str+\".csv\"\n",
    "    predict_set = pd.read_csv(input_filename)\n",
    "    print(\"syj predict_set\",predict_set.columns)\n",
    "    if(validate_or_test_flag):\n",
    "        predict_set_temp = predict_set.iloc[:,3:]    #这一行要去除掉\n",
    "    else:\n",
    "        predict_set_temp = predict_set.iloc[:,4:]\n",
    "    print(\"syj predict_set_temp\",predict_set_temp.columns)\n",
    "    print(\"syj predict_set_temp\",predict_set_temp.head(5))\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(predict_set_temp.values)\n",
    "    predict_set_temp_values = scaler.transform(predict_set_temp.values)\n",
    "    predict_set_scale = pd.DataFrame(predict_set_temp_values) \n",
    "    #prdict\n",
    "    train_model_predict_start_time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print('train model predict start time:',train_model_predict_start_time)  \n",
    "    if(algorithm == 'LR'):\n",
    "        #load model\n",
    "        save_path_name=model_save_path+\"lr_\"+\"train_model.m\"\n",
    "        print(\"loading model name\",save_path_name)\n",
    "        modle_lr = joblib.load(save_path_name) \n",
    "        if(use_predict_proba):\n",
    "            res = modle_lr.predict_proba(predict_set_scale.values)\n",
    "            train_model_predict_finish_time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print('train model predict finish time:',train_model_predict_finish_time)      \n",
    "            #join reference data set   \n",
    "            res = pd.DataFrame(res)\n",
    "            predict_data_set = pd.concat([predict_set,res],axis=1)\n",
    "            predict_data_set.rename(columns={0:'predict_label_not_buy',1:'predict_label_buy'},inplace=True)\n",
    "\n",
    "            predict_data_set = predict_data_set[['item_id','user_id','predict_label_buy']]\n",
    "            predict_data_set_buy = predict_data_set[predict_data_set['predict_label_buy'] >0.8]\n",
    "            predict_data_set_buy[['user_id','item_id','predict_label_buy']].to_csv(\"../DataSet/tianchi_mobile_recommendation_predict_proba.csv\",index=None)\n",
    "        else:   \n",
    "            res = modle_lr.predict(predict_set_scale.values)\n",
    "            train_model_predict_finish_time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print('train model predict finish time:',train_model_predict_finish_time)      \n",
    "            #join reference data set   \n",
    "            res = pd.DataFrame(res)\n",
    "            predict_data_set = pd.concat([predict_set,res],axis=1)\n",
    "            predict_data_set.rename(columns={0:'predict_label'},inplace=True)\n",
    "\n",
    "            predict_data_set = predict_data_set[['item_id','user_id','predict_label']]\n",
    "            predict_data_set_buy = predict_data_set[predict_data_set['predict_label']==1]\n",
    "        predict_data_set_buy[['user_id','item_id']].to_csv(\"../DataSet/tianchi_mobile_recommendation_predict.csv\",index=None)\n",
    "    elif(algorithm == 'RF'):\n",
    "        #load model\n",
    "        save_path_name=model_save_path+\"RF_\"+\"train_model.m\"\n",
    "        print(\"loading model name\",save_path_name)\n",
    "        modle_rf = joblib.load(save_path_name) \n",
    "        res = modle_rf.predict(predict_set_scale.values)\n",
    "        train_model_predict_finish_time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print('train model predict finish time:',train_model_predict_finish_time)      \n",
    "        #join reference data set   \n",
    "        res = pd.DataFrame(res)\n",
    "        predict_data_set = pd.concat([predict_set,res],axis=1)\n",
    "        predict_data_set.rename(columns={0:'predict_label'},inplace=True)\n",
    "\n",
    "        predict_data_set = predict_data_set[['item_id','user_id','predict_label']]\n",
    "        predict_data_set_buy = predict_data_set[predict_data_set['predict_label']==1]\n",
    "        #\n",
    "        item_table = pd.read_csv('../DataSet/tianchi_fresh_comp_train_item.csv')\n",
    "        predict_data_set_buy = predict_data_set_buy[predict_data_set_buy.item_id.isin(list(item_table.item_id))]\n",
    "        #\n",
    "        predict_data_set_buy[['user_id','item_id']].to_csv(\"../DataSet/tianchi_mobile_recommendation_predict.csv\",index=None)\n",
    "        print(\"random forest end\")\n",
    "    elif(algorithm == 'GBDT'):\n",
    "        #load model\n",
    "        save_path_name=model_save_path+\"GBDT_\"+\"train_model.m\"\n",
    "        print(\"loading model name\",save_path_name)\n",
    "        modle_gbdt = joblib.load(save_path_name) \n",
    "        res = modle_gbdt.predict(predict_set_scale.values)\n",
    "        res_prob = modle_gbdt.predict_proba(predict_set_scale.values)###\n",
    "        res = (modle_gbdt.predict_proba(predict_set_scale.values)[:,1] > prob_value).astype(int)\n",
    "       \n",
    "        print(\"cged\")\n",
    "        print(res_prob)\n",
    "        print(modle_gbdt.classes_)\n",
    "        print(\"again\")\n",
    "        train_model_predict_finish_time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print('train model predict finish time:',train_model_predict_finish_time)   \n",
    "        \n",
    "        #join reference data set   \n",
    "        res = pd.DataFrame(res)\n",
    "        \n",
    "        print(\"before predict:\",predict_set_scale.shape,\"after predict\",res.shape)\n",
    "        \n",
    "        predict_data_set = pd.concat([predict_set,res],axis=1)    #. 确认这条语句的执行结果是否正确\n",
    "        predict_data_set.rename(columns={0:'predict_label'},inplace=True)\n",
    "\n",
    "        predict_data_set = predict_data_set[['item_id','user_id','predict_label']]\n",
    "        predict_data_set_buy = predict_data_set[predict_data_set['predict_label']==1]\n",
    "        #\n",
    "        item_table = pd.read_csv('../DataSet/tianchi_fresh_comp_train_item.csv')\n",
    "        predict_data_set_buy = predict_data_set_buy[predict_data_set_buy.item_id.isin(list(item_table.item_id))]\n",
    "        predict_data_set_buy[['user_id','item_id']].to_csv(\"../DataSet/tianchi_mobile_recommendation_predict.csv\",index=None)\n",
    "        print('modle_gbdt.feature_importances_:',modle_gbdt.feature_importances_)\n",
    "        #\n",
    "        f1_scores=[]\n",
    "        precision_score=[]\n",
    "        recall=[]\n",
    "        prob_values = [0.1,0.2,0.3,0.35,0.38,0.4,0.42,0.45,0.5,0.6,0.7]\n",
    "        if(validate_or_test_flag):\n",
    "            pass\n",
    "        else:\n",
    "            for prob_value in [0.1,0.2,0.3,0.35,0.38,0.4,0.42,0.45,0.5,0.6,0.7]:\n",
    "                predict_set_temp = (res_prob[:,1] > prob_value).astype(int)\n",
    "                f1_scores.append(metrics.f1_score(predict_set[['label',]], predict_set_temp))\n",
    "                precision_score.append(metrics.precision_score(predict_set[['label',]], predict_set_temp))\n",
    "                recall.append(metrics.recall_score(predict_set[['label',]], predict_set_temp))\n",
    "                # plot the result\n",
    "            f1 = plt.figure(1)\n",
    "            plt.plot(prob_values, f1_scores, label=\"f1_scores\")\n",
    "            plt.plot(prob_values, precision_score, label=\"precision_score\")\n",
    "            plt.plot(prob_values, recall, label=\"recall\")\n",
    "            plt.xlabel('NP ratio')\n",
    "            plt.ylabel('f1_score')\n",
    "            plt.title('prob_values of f1_score - GBDT')\n",
    "            plt.legend(loc='upper right') # 标签位置\n",
    "            plt.grid(True, linewidth=0.5)\n",
    "            plt.show()\n",
    "                \n",
    "            '''f1_scores=metrics.f1_score(predict_set[['label',]], res)\n",
    "            precision_score=metrics.precision_score(predict_set[['label',]], res)\n",
    "            recall=metrics.recall_score(predict_set[['label',]], res)\n",
    "            print('GBDT f1, accurency, recall:',f1_scores,precision_score,recall)'''\n",
    "        print(\"GBDT end\")        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Unit Test As Follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "window_size=7\n",
    "end = datetime.datetime(2014,12,18,0,0,0)\n",
    "start = end - datetime.timedelta(days=window_size-1)   \n",
    "end_day_str = str(end.date())\n",
    "input_filename = \"../DataSet/Feature/\"+\"part_data_uiclf\"+end_day_str+\".csv\"\n",
    "predict_set = pd.read_csv(input_filename)\n",
    "predict_set.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "window_size=7\n",
    "end = datetime.datetime(2014,12,18,0,0,0)\n",
    "start = end - datetime.timedelta(days=window_size-1)   \n",
    "#修改日期时注意这个train_or_predict_flag的选择，true代预测test data 12.19；false代表生成 validate data12.18\n",
    "predict(start, end, 'GBDT', False, False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0.4     GBDT f1, accurency, recall: 0.06499470151889791 0.04595404595404595 0.11097708082026538\n",
    "0.35   GBDT f1, accurency, recall: 0.06783369803063456 0.04578059071729958 0.13088057901085645\n",
    "0.3     GBDT f1, accurency, recall: 0.06595196091735649 0.042549465942917176 0.146562123039807\n",
    "最近3天的数据。\n",
    "0.4    GBDT f1, accurency, recall: 0.0858327237259205 0.07204257060990585 0.10615199034981906\n",
    "0.35   GBDT f1, accurency, recall: 0.08695652173913045 0.06723796967699407 0.12303980699638119\n",
    "0.3     GBDT f1, accurency, recall: 0.08340888485947416 0.05963183821623023 0.13872135102533173\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
